---
title: "Estrategias evolutivas"
output: html_notebook
---


# Introducción

Los algoritmos del área de la computación evolutiva tratan de resolver problemas de diversa índole sin que se necesite asumir demasiadas hipótesis para su aplicación. Los algoritmos más clásicos de métodos numéricos para la resolución de problemas de optimización suelen tener unas hipótesis asociadas que en la práctica son difíciles de cumplir. Por ejemplo, el algoritmo del descenso del gradiente exige que la función sobre la que se aplique sea diferenciable. Otras hipótesis pueden ser la concavidad o convexidad, continuidad, problemas con los mínimos o máximos locales, etcétera. 

Sin embargo, en funciones que pueden ser muy complejas o en problemas en los que se modelice la realidad, es frecuente que estas hipótesis no puedan aplicarse y, por tanto, haya que buscar algoritmos lo más *agnósticos* posibles. Es el caso de las estrategias evolutivas. 

# Definición del problema

El objetivo es el de evaluar el comportamiento de estrategias evolutivas en la optimización de:

- **función suma de powell**.
- **función Nº4 de Xin-She Yang**.

Estas dos funciones son lo suficientemente complejas como para que no se puedan aplicar algoritmos habituales de métodos numéricos.

## Función suma de Powell

La función suma de Powel está definida por la expresión

$$
f(\bar{x}) = \sum_{i = 1}^n |x_i|^{i+1}
$$

donde $\bar{x} = (x_1, x_2, \ldots, x_n)$, siendo $-1 \leq x_i \leq 1$, $i = 1, \ldots,n$. Esta función tiene como propiedades ser:

- multi-dimensional
- continua
- convexa
- no-diferenciable
- unimodal

Podemos representar la función en el caso en el que $\bar{x} \in \mathbb{R}^2$.

```{r echo=FALSE}
library(plot3D)
source("funciones_a_optimizar.R")

x <- seq(-1, 1, length.out=50)
y <- x
M <- mesh(x, y)

alpha <- M$x
beta <- M$y

a <- cbind(as.vector(alpha), as.vector(beta))

z <- apply(a, 1, suma_powell)

b <- matrix(z, nrow = 50, ncol = 50)

par(mfrow = c(1, 2))
surf3D(x      = alpha,
       y      = beta,
       z      = b,
       colkey = FALSE,
       bty    = "b2",
       main   = "Función suma de Powell ",
       theta = 90)

surf3D(x      = alpha,
       y      = beta,
       z      = b,
       #colkey = FALSE,
       bty    = "b2",
       #main   = "Función suma de Powell ",
       theta = 90,
       phi = 0)
par(mfrow = c(1, 1))
```

Esta función presenta un mínimo global que se alcanza en $\bar{x}^* = (0,0)$ tomando $f(\bar{x}^* ) = 0$.

## Función Nº 4 de Xin-She Yang

La función Nº 4 de Xin-She Yang tiene como expresión

$$
f(\bar{x}) = \left( \sum_{i=1}^n sin^2(x_i) - exp\left(-\sum_{i=1}^nx_i^2\right)\right)\cdot exp\left(-\sum_{i=1}^nsin^2(\sqrt{|x_i|})\right)
$$

Esta función es:

- No convexa.
- No diferenciable.


```{r echo=FALSE}

x <- seq(-10, 10, length.out=50)
y <- x
M <- mesh(x, y)

alpha <- M$x
beta <- M$y

a <- cbind(as.vector(alpha), as.vector(beta))

z <- apply(a, 1, xin_she_yang_4)

b <- matrix(z, nrow = 50, ncol = 50)

par(mfrow = c(1, 2))
surf3D(x      = alpha,
       y      = beta,
       z      = b,
       colkey = FALSE,
       bty    = "b2",
       main   = "Función suma de Powell ")

surf3D(x      = alpha,
       y      = beta,
       z      = b,
       colkey = FALSE,
       bty    = "b2",
       #main   = "Función Nº 4 de Xin-She Yang",
       theta = 90,
       phi = 0)
par(mfrow = c(1, 1))
```


En el caso de $\mathbb{R}^2$, se ve que existe una gran cantidad de mínimos locales pero solo un mínimo global que se alcanza en $\bar{x}^* = (0,0)$ tomando $f(\bar{x}^* ) = -1$.

# Descripción de Estrategia Evolutiva

# Implementación


# Discusión de resultados

A continuación se detallan los resultados obtenidos al aplicar los algoritmos de estrategias evolutivas sobre las funciones de suma de Powell y la Nº 4 de Xin-She Yang.

Para ambas funciones se van a ejecutar estrategias evolutivas cuyas configuraciones serán:

1. problema con dimensión $5$: $\bar{x} = (x_1, \ldots, x_5)$.
2. 

## Función suma de Powell

### Configución 1

Estas ejecuciones se han realizado con las siguientes características:

- mutación no correlacionada de paso único,
- estrategia de selección de supervivientes de tipo $(\lambda, \mu) = (30, 200)$,
- $1000$ generaciones (iteraciones),
- $20$ ejecuciones independientes

A contunuación se muestra la gráfica de progreso obtenida de la ejecución del algoritmo:

```{r echo=FALSE}
source("R/graficos.R")

resultados <- readRDS("resultados/powell_3_,")
resultados <- grafico_progreso(resultados)
resultados %>%
  #filter(iteracion <= 20) %>%
  ggplot(aes(x = iteracion,
             y = fitness,
             group = id_ejecucion)) +
  geom_line() + 
  labs(caption = "Gráfico de progreso")
```

La escala del gráfico dificulta obtener conclusiones. Si hacemos *zoom* sobre las 20 primeras iteraciones del algoritmo, comprobamos el abrupto descenso que sucede en las primeras 10 iteraciones.

```{r}
resultados %>%
  filter(iteracion <= 20) %>%
  ggplot(aes(x = iteracion,
             y = fitness,
             group = id_ejecucion)) +
  geom_line() + 
  labs(caption = "Gráfico de progreso")
```

```{r}
resultados %>%
  filter(iteracion > 20, iteracion < 50) %>%
  ggplot(aes(x = iteracion,
             y = fitness,
             group = id_ejecucion)) +
  geom_line() + 
  labs(caption = "Gráfico de progreso")
```

En la representación del progreso de la iteración $20$ hasta la $50$, el algoritmo comienza a comportarse de manera herrática, comportamiento que también suede si representamos de la iteración $20$ hasta las 1000 

```{r}
resultados %>%
  filter(iteracion > 20) %>%
  ggplot(aes(x = iteracion,
             y = fitness,
             group = id_ejecucion)) +
  geom_line() + 
  labs(caption = "Gráfico de progreso")
```

Para obtener una mejor visión del problema, vamos a representar la ganancia porcentual entre dos iteraciones, es decir,

$$
ganacia(i) = \frac{f_i - f_{i-1}}{f_{i-1}}
$$

```{r}
resultados %>% 
  group_by(id_ejecucion) %>% 
  mutate(inc_fitness = abs(fitness - lag(fitness))/lag(fitness)) %>% 
  filter(iteracion <= 50) %>%
  ggplot(aes(x = iteracion,
             y = inc_fitness
             )) +
  geom_line(aes(group = id_ejecucion), alpha = 0.5) + 
  geom_smooth(color = "firebrick") +
  labs(caption = "Gráfico de progreso")
```

En un problema en el que desconocemos las características de la función de *fitness* que queremos optimizar, es imposible poder saber lo precisa que es la solución propuesta. Sin embargo, en nuestro caso conocemos las propiedades de la función suma de Powell, y, al conocer que el mínimo se alcanza en $\bar{x}^* = \bar{0}$ siendo $f(\bar{x}^*) = 0$, podemos saber cómo de próxima es la solución obtenida a la solución real y poder definir un umbral de error para el que podemos considerar que el problema ha sido resuelto. La decisión del umbral es completamente arbitraria y dependerá del problema que se tenga entre manos. Por supuesto, cuanto menor sea la tolerancia permitida, más tardará el algoritmo en satisfacer la condición. En este caso, consideraremos que el algoritmo ha solucionado el problema cuando se obtenga que $|f(x)| \leq 10^{-6}$

En el orden de 5 iteraciones, el algoritmo consigue obtener un valor estable. Si hacemos *zoom* sobre lo que sucede a partir de la iteración $5$:

```{r}
grafico_progreso(resultados) %>%
  filter(iteracion >= 10, iteracion <= 50) %>%
  ggplot(aes(x = iteracion,
             y = fitness,
             group = id_ejecucion)) +
  geom_line() + 
  labs(caption = "Gráfico de progreso")
```


## Función Nº 4 de Xin-She Yang


# Discusión

Las estrategias evolutivas nos sirven para resolver problemas complejos. No obstante, si la función o el problema a optimizar cumple las hipótesis de estos algoritmos, estos tendrán un comportamiento mejor que el que pueda tener una estrategia evolutiva puesto que se aprovechan de la estructura de la función para mejorar el proceso de optimización. Por ello, hay que tener en cuenta la naturaleza del problema que se tiene entre manos para saber qué tipo de algoritmo sería el adecuado.